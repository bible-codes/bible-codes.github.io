# Predictive ELS: Time-Series News → Torah Proximity → ML Prediction Pipeline

## Context

**Problem**: The ELS proximity matrix around seed keywords contains hundreds of candidate terms (words, names, dates). Currently all but dictionary-validated, high-significance terms are discarded. But when a news story unfolds over time, newly-revealed details (names, places, dates) sometimes match previously-rejected candidates. This wasted signal is a training opportunity.

**Goal**: Build a system that:
1. Takes news story keywords at time t, runs ELS proximity search, stores ALL candidates (accepted + rejected)
2. As story develops (t+1, t+2...), checks new keywords against stored candidate pools
3. Tracks hit/miss rates across stories to train an ML model for better candidate selection
4. Predicts next-most-plausible keywords by ranking ELS candidates with learned weights
5. Uses verse context (verses crossing multiple ELS terms) for narrative coherence

**Key insight from user**: Tsirufim (permutation engine) should be integrated as a keyword expansion engine — ELS finds terms, Tsirufim generates related/permuted words from those terms, which feed back as new ELS seeds. This closes the discovery loop and subsumes the planned anagram solver.

**Input approach**: Manual Hebrew keyword entry initially. Future automation: scrape Hebrew news media (Ynet, Walla, etc.) for native spellings of names, events, places — avoids translation artifacts and gives exact terms the public encounters.

---

## Architecture Overview

```
News Keywords (t=0)
       │
       ▼
┌─────────────────┐     ┌──────────────────┐
│  ELS Proximity   │────▶│  Full Candidate   │
│  Search Engine   │     │  Pool (stored)    │
│  (els-index.js)  │     │  accepted+rejected│
└─────────────────┘     └────────┬─────────┘
       │                         │
       ▼                         ▼
┌─────────────────┐     ┌──────────────────┐
│  Tsirufim        │────▶│  Expanded Terms   │
│  Permutation     │     │  (feed back to    │
│  + Scoring       │     │   ELS search)     │
└─────────────────┘     └──────────────────┘
       │                         │
       ▼                         ▼
┌─────────────────┐     ┌──────────────────┐
│  Verse Context   │     │  New Keywords     │
│  Builder         │     │  (t+1 news)       │
│  (cross-term     │     │  → retrovalidate  │
│   verse text)    │     │    against pool    │
└─────────────────┘     └────────┬─────────┘
       │                         │
       ▼                         ▼
┌──────────────────────────────────────────┐
│  ML Scoring Model (in-browser)            │
│  Features: z-score, proximity, embedding  │
│  similarity, dictionary match, coherence, │
│  tsirufim score, verse overlap            │
│  Target: was_relevant (0/1)               │
└──────────────────────────────────────────┘
```

---

## Phase 1: Story Timeline + Full Candidate Extraction

**Deliverables**: New page `predictive-els.html` + engine `engines/predictive-els.js`

### 1A. Data Model — Story Timeline

```javascript
// IndexedDB store: "stories"
{
  id: AUTO_INCREMENT,
  title: "Story headline",
  createdAt: ISO_DATE,
  eventType: "conflict"|"movement"|"speech"|..., // maps to scoring.js anchors
  timesteps: [
    {
      t: 0,
      date: ISO_DATE,
      keywords: ["מלחמה", "עזה", "חמאס"],   // Hebrew keywords at this timestep
      source: "manual" | "news-paste",        // how keywords were entered
      notes: "Initial story breakout"
    },
    {
      t: 1,
      date: ISO_DATE,
      keywords: ["רפיח", "מנהרות"],           // newly revealed details
      source: "manual",
      notes: "New details emerged"
    }
  ],
  status: "active" | "archived"
}
```

### 1B. Data Model — Candidate Pool

```javascript
// IndexedDB store: "candidate_pools"
{
  id: AUTO_INCREMENT,
  storyId: FK → stories.id,
  timestep: 0,                               // which timestep generated this pool
  searchCenter: 45000,                        // Torah position (centroid)
  searchRadius: 2000,                         // proximity radius used
  generatedAt: ISO_DATE,
  candidates: [
    {
      word: "רפיח",
      positions: [{pos: 45123, skip: 17}, ...],
      minDistance: 42,                         // to search center
      zScore: 3.2,                            // statistical significance
      inDictionary: true,
      dictionarySource: "unified",
      root: "רפח",
      embedding: Float32Array,                // 64D feature vector (cached ref)
      contextualScore: 0.72,                  // from ContextualScorer
      coherenceScore: 0.45,
      tsirufimExpanded: false,                // was this generated by Tsirufim?
      tsirufimSource: null,                   // parent word if expanded
      status: "accepted" | "rejected" | "unscored",
      rejectionReason: "low_significance" | "not_in_dictionary" | "low_coherence" | null,

      // FILLED IN LATER by retrospective validation:
      wasRelevant: null,                      // true/false/null (training label)
      matchedAtTimestep: null,                // which t did this become relevant?
      matchType: null                         // "exact" | "root" | "semantic"
    }
  ]
}
```

### 1C. Engine — `engines/predictive-els.js` (~400 lines)

Core class composing existing services:

```javascript
import { getElsIndexService } from './els-index.js';
import { getDictionaryService } from './dictionary-service.js';
import { getScorer } from './tsirufim/scoring.js';
import { getPermutationGenerator } from './tsirufim/permutations.js';
import { getEmbeddings } from './tsirufim/embeddings.js';

export class PredictiveELSEngine {

  // Initialize all sub-engines
  async initialize()

  // PHASE 1 CORE: Extract all candidates from proximity matrix
  // Uses: elsIndex.findNearby() + elsIndex.discoverCluster()
  // Returns: full candidate array (accepted + rejected, with reasons)
  async extractCandidates(keywords, options = {radius, minLength, maxResults})

  // Score candidates using existing ContextualScorer
  // Uses: scorer.scoreCandidates() with story keywords as situation
  async scoreCandidates(candidates, keywords, eventType)

  // TSIRUFIM INTEGRATION: Expand a candidate word via permutation
  // Uses: permutationGenerator.generateSubwords() + scorer
  // Returns: expanded terms with scores, marked tsirufimExpanded=true
  async expandViaTsirufim(word, contextKeywords, options)

  // Full pipeline: extract → score → expand top terms → re-search ELS
  async runDiscoveryPipeline(keywords, eventType, options)
}
```

### 1D. Tsirufim ↔ ELS Integration Loop

This is the key new connection:

```
ELS findNearby("חמאס", 2000)
  → candidate: "משח" (found at skip=47, distance=340)
  → Tsirufim expand "משח":
    → generateSubwords("משח") → ["שמח", "חמש", "משח"]
    → score each against story context
    → "שמח" scores low (joy ≠ conflict context) → reject
    → "חמש" scores medium (five/weapon) → keep
  → Feed "חמש" back to ELS as new seed
  → ELS findNearby position of "חמש"
    → discovers new cluster of terms near it
```

**Reused functions**:
- `PermutationGenerator.generateSubwords(letters)` — generates all sub-permutations
- `PermutationGenerator.generateAnagrams(letters)` — full-length permutations
- `ContextualScorer.scoreCandidates(candidates, situation)` — rank by relevance
- `ElsIndexService.findNearby(pos, radius)` — proximity discovery
- `ElsIndexService.significanceScore(word)` — z-score filtering

### 1E. UI — Story Input Panel

```
┌─────────────────────────────────────────────────┐
│  Predictive ELS - Story Timeline                │
├─────────────────────────────────────────────────┤
│  Story: [________________________________]      │
│  Event Type: [conflict ▼]                       │
│                                                 │
│  ── Timestep 0 (2026-02-14) ──────────────────  │
│  Keywords: [מלחמה עזה חמאס    ] ⌨              │
│  Notes:    [Initial breakout   ]                │
│  [+ Add Timestep]                               │
│                                                 │
│  [Run Discovery]  [Save Story]                  │
├─────────────────────────────────────────────────┤
│  CANDIDATE POOL (t=0) — 347 terms found         │
│  ┌─────┬───────┬──────┬───────┬────────┬──────┐ │
│  │ # │ Word  │ zScr │ Prox  │ Score  │ Dict │ │
│  ├─────┼───────┼──────┼───────┼────────┼──────┤ │
│  │ 1 │ רפיח  │ 4.1  │ 42    │ 0.89   │ ✓    │ │
│  │ 2 │ מנהרה │ 3.7  │ 118   │ 0.82   │ ✓    │ │
│  │ 3 │ qwrty │ 2.1  │ 450   │ 0.31   │ ✗    │ │
│  │...                                         │ │
│  └────────────────────────────────────────────┘ │
│  [Show Rejected] [Expand via Tsirufim] [Matrix] │
└─────────────────────────────────────────────────┘
```

---

## Phase 2: Retrospective Validation + Hit Tracking

### 2A. Validation Engine

When user adds timestep t+1 with new keywords:
1. Load candidate pool from t=0
2. For each new keyword, check against ALL stored candidates:
   - **Exact match**: keyword === candidate.word
   - **Root match**: same שורש (via roots.js extractRoot)
   - **Semantic match**: embeddingSimilarity > threshold (e.g., 0.8)
3. Mark matched candidates: `wasRelevant = true`, `matchedAtTimestep = t+1`
4. Track hit rate: `hits / totalCandidates` per timestep

```javascript
// In predictive-els.js
async retrovalidate(storyId, newTimestep) {
  const pools = await db.getCandidatePools(storyId);  // all prior timestep pools
  const newKeywords = newTimestep.keywords;
  const hits = [];

  for (const pool of pools) {
    for (const candidate of pool.candidates) {
      const match = await this.matchCandidate(candidate, newKeywords);
      if (match) {
        candidate.wasRelevant = true;
        candidate.matchedAtTimestep = newTimestep.t;
        candidate.matchType = match.type;  // exact/root/semantic
        hits.push({ candidate, match });
      }
    }
    await db.updateCandidatePool(pool);  // persist labels
  }
  return hits;
}
```

### 2B. Hit Rate Dashboard

```
Timestep 0 → 1:  12 / 347 candidates matched (3.5%) — 4 exact, 5 root, 3 semantic
Timestep 0 → 2:  19 / 347 candidates matched (5.5%) — cumulative
Timestep 1 → 2:   8 / 212 candidates matched (3.8%)
Among REJECTED candidates: 3 / 198 matched (1.5%) ← these are the "missed signals"
```

### 2C. Verse Context Builder

When multiple ELS terms from different keywords share the same verse:

```javascript
// Reuse existing infrastructure:
// - charDatabase (position → verse mapping)
// - getVerseTextByKey(key) (verse text retrieval)
// - buildHitVerseMap() (verse → positions mapping)

async buildNarrativeContext(candidatePool) {
  // Find verses that contain 2+ candidate terms
  const verseTermMap = new Map();  // verseKey → Set<candidateWord>
  for (const c of candidatePool.candidates.filter(c => c.status === 'accepted')) {
    for (const occ of c.positions) {
      const verseKey = getVerseKey(occ.pos);  // existing function
      if (!verseTermMap.has(verseKey)) verseTermMap.set(verseKey, new Set());
      verseTermMap.get(verseKey).add(c.word);
    }
  }
  // Return verses with 2+ terms — these provide narrative context
  return [...verseTermMap.entries()]
    .filter(([k, terms]) => terms.size >= 2)
    .map(([key, terms]) => ({
      verse: key,
      text: getVerseTextByKey(key),
      crossingTerms: [...terms],
      termCount: terms.size
    }))
    .sort((a, b) => b.termCount - a.termCount);
}
```

---

## Phase 3: Feature Engineering + Training Signal

### 3A. Feature Vector per Candidate (for ML)

Each candidate → feature vector for supervised learning:

| # | Feature | Source | Type |
|---|---------|--------|------|
| 1 | z-score (significance) | `elsIndex.significanceScore()` | float |
| 2 | min proximity distance | `findNearby().minDistance` | float |
| 3 | occurrence count | `elsIndex.getOccurrenceCount()` | int |
| 4 | word length | `word.length` | int |
| 5 | in dictionary (any) | `dictService.isKnownWord()` | bool |
| 6 | dictionary source count | how many dict sources match | int |
| 7 | era = biblical | `dictService.searchByEra()` | bool |
| 8 | has extractable root | `rootExtractor.extractRoot()` | bool |
| 9 | root confidence | rootData.confidence | float |
| 10 | contextual score | `scorer.scoreCandidates()` | float |
| 11 | coherence score | scorer coherence component | float |
| 12 | event anchor alignment | scorer anchor component | float |
| 13 | semantic drift | scorer drift component | float |
| 14 | embedding similarity to context | cosine(candidate, centroid) | float |
| 15 | tsirufim-expanded | was generated via permutation | bool |
| 16 | verse cross-count | how many other terms share verses | int |
| 17 | gematria standard | from embedding features | int |
| 18 | centroid distance | distance to cluster centroid | float |

**Label**: `wasRelevant` (0 or 1) — set during Phase 2 retrovalidation

### 3B. Training Data Accumulation

```javascript
// IndexedDB store: "training_data"
{
  id: AUTO_INCREMENT,
  storyId: FK,
  timestep: int,
  features: Float32Array(18),  // normalized feature vector
  label: 0 | 1,               // wasRelevant
  word: string,                // for debugging
  createdAt: ISO_DATE
}
```

Accumulates across all stories. Even 5-10 stories with ~300 candidates each = 1500-3000 training samples.

---

## Phase 4: ML Prediction Model

### 4A. Training Architecture: External, Not In-Browser

Training will NOT be done in the browser. The accumulated training data (candidate features + wasRelevant labels) will be **exported** from the PWA and used to train a model externally. The expectation is that this will ultimately require a **large language model** (fine-tuned or purpose-trained) rather than a simple classifier, because:

- The feature space is rich and contextual (Hebrew morphology, Torah positional semantics, news narrative structure)
- Candidate selection is fundamentally a language understanding task (is this word relevant to this story?)
- The model needs to generalize across story types, not just memorize feature thresholds
- Verse context integration requires sequence understanding, not just tabular features

**Training pipeline (external)**:
1. PWA exports training data as JSON/CSV (stories + candidate pools + labels)
2. Training happens on GPU server (Python: PyTorch/HuggingFace, or fine-tune existing Hebrew LLM)
3. Model options (escalating complexity):
   - **Baseline**: XGBoost/Random Forest on 18 feature vectors (fast, interpretable, good starting point)
   - **Mid-tier**: Fine-tuned Hebrew BERT (AlephBERT/HeBERT) on candidate-in-context classification
   - **Full**: Custom LLM trained on ELS candidate → relevance prediction with Torah verse context
4. Trained model exported as:
   - ONNX weights for in-browser inference (via ONNX.js or TF.js)
   - Or: API endpoint if model too large for browser
5. PWA loads trained model weights and runs **inference only** in-browser

**PWA responsibilities**:
- Collect and store training data (Phases 1-3)
- Export training data for external training
- Import trained model weights
- Run inference on new candidates using imported model
- Display predictions in UI

### 4B. Data Export Format

```javascript
// Export from PWA for external training
{
  version: "1.0",
  exportDate: ISO_DATE,
  stories: [...],           // full story timelines
  candidatePools: [...],    // all candidates with features + labels
  trainingData: [           // flattened feature vectors
    { features: [18 floats], label: 0|1, word: "...", storyId: N, context: "..." }
  ],
  metadata: {
    totalStories: N,
    totalCandidates: N,
    positiveRate: 0.035,    // % of candidates that were relevant
    featureNames: ["zScore", "minDistance", ...]
  }
}
```

### 4C. Inference Mode UI (in PWA, using trained model)

After model is trained externally and weights imported:
- User enters new story keywords
- System runs ELS discovery pipeline
- Each candidate scored by trained model (inference only)
- Candidates ranked by predicted probability
- Top candidates highlighted as "predicted relevant"

```
PREDICTIONS (model v2, trained on 45 stories)
┌───┬────────┬──────────┬───────────┬──────────┐
│ # │ Word   │ ML Score │ z-Score   │ Status   │
├───┼────────┼──────────┼───────────┼──────────┤
│ 1 │ רפיח   │ 0.91     │ 4.1       │ PREDICT  │
│ 2 │ מנהרה  │ 0.84     │ 3.7       │ PREDICT  │
│ 3 │ חטוף   │ 0.78     │ 2.8       │ PREDICT  │
│ 4 │ נשק    │ 0.62     │ 1.9       │ possible │
│ 5 │ qwrty  │ 0.12     │ 0.4       │ unlikely │
└───┴────────┴──────────┴───────────┴──────────┘
```

---

## Phase 5: Verse Context + Narrative Building

Cross-term verse analysis provides interpretive scaffolding:

- When ELS terms from the story appear in the same verse, that verse text gives narrative context
- Display crossing verses alongside predictions
- Use verse text as additional context for Tsirufim expansion (situation embedding includes verse words)

---

## File Plan

| File | Type | Est. Lines | Purpose |
|------|------|-----------|---------|
| `predictive-els.html` | New page | ~800 | Story timeline UI, candidate table, prediction display |
| `engines/predictive-els.js` | New engine | ~500 | Discovery pipeline, retrovalidation, feature extraction |
| `engines/model-inference.js` | New engine | ~150 | Load trained model weights, run inference on candidates |
| `db/predictive-schema.js` | New DB schema | ~100 | IndexedDB stores: stories, candidate_pools, training_data |
| `bible-codes.html` | Modify | +10 | Add link to predictive-els page |

**Total new code**: ~1,550 lines across 4 new files + minor modification to 1 existing file.

**Reused (not modified)**:
- `engines/els-index.js` — findNearby, discoverCluster, significanceScore, computeProximityMatrix
- `engines/dictionary-service.js` — isKnownWord, lookup, getRoot
- `engines/tsirufim/scoring.js` — scoreCandidates, buildSituationEmbedding
- `engines/tsirufim/embeddings.js` — getEmbedding, semanticSimilarity, getCentroid
- `engines/tsirufim/permutations.js` — generateSubwords, generateAnagrams
- `engines/tsirufim/clustering.js` — clusterKMeans (for grouping candidates)
- `engines/roots.js` — extractRoot (for root-match validation)
- `bible-codes.html` — charDatabase, getVerseKey, getVerseTextByKey (verse context)

---

## Implementation Order

### Phase 1 (core, ~6-8 hours)
1. Create `db/predictive-schema.js` — IndexedDB stores
2. Create `engines/predictive-els.js` — discovery pipeline with Tsirufim loop
3. Create `predictive-els.html` — story input + candidate table UI
4. Wire up ELS → Tsirufim → ELS feedback loop

### Phase 2 (validation, ~3-4 hours)
5. Add timestep management UI (add/edit timesteps)
6. Implement `retrovalidate()` — match new keywords against stored pools
7. Hit rate dashboard display
8. Verse context builder (crossing-verse extraction)

### Phase 3 (feature engineering, ~3-4 hours)
9. Feature extraction pipeline (18 features per candidate)
10. Training data accumulation in IndexedDB
11. Training data export (JSON/CSV for external ML training)

### Phase 4 (ML model integration, ~3-4 hours)
12. External: Train model on exported data (Python, GPU — separate repo/project)
13. Create `engines/model-inference.js` — load trained weights, run inference in PWA
14. Prediction mode UI (ranked candidates with model scores)
15. Model weight import/versioning in IndexedDB

### Phase 5 (polish, ~2-3 hours)
15. Verse narrative context panel
16. Historical story import/export (JSON)
17. Dashboard with cross-story statistics

---

## Future: Automated Hebrew News Pipeline

Eventually automate the training loop:
- Scrape Hebrew news media (Ynet, Walla, Maariv, etc.) for breaking stories
- Extract keywords natively in Hebrew (correct common spellings for names/events)
- Auto-generate timesteps as story develops over days/weeks
- Feed into ELS pipeline automatically
- Accumulate training data at scale without manual entry

---

## Verification Plan

1. **Unit test**: Create a test story with known keywords at t=0, run discovery, verify candidate pool stored
2. **Retrovalidation test**: Add t=1 keywords that include a known ELS term, verify it gets marked as hit
3. **Tsirufim loop test**: Verify that expanding a term via permutations generates valid ELS-searchable words
4. **ML test**: Accumulate mock training data, train model, verify predictions are ordered sensibly
5. **End-to-end**: Load a historical news story, walk through all timesteps, verify hit rate tracking
6. **Verse context test**: Verify crossing-verse detection for terms that share Torah verses
7. **Persistence test**: Save story → reload page → verify story and pools persist in IndexedDB
